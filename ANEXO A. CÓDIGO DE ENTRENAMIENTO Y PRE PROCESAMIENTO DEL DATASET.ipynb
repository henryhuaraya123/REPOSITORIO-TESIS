{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395cc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# --- 1. Función Auxiliar: Definición de parámetros para Fine-Tuning ---\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "# --- 2. Función Clave: Inicialización de Modelos con Tamaños Óptimos ---\n",
    "# Nota: Todos los tamaños de entrada son los estándares recomendados por las especificaciones de ImageNet.\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "\n",
    "    if model_name == \"VGG11-BN\":\n",
    "        \"\"\" VGG11_bn (Input 224) \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "\n",
    "    elif model_name == \"DenseNet-201\":\n",
    "        \"\"\" DenseNet-201 (Input 224) \"\"\"\n",
    "        model_ft = models.densenet201(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "\n",
    "    elif model_name == \"ResNet-101\":\n",
    "        \"\"\" ResNet-101 (Input 224) \"\"\"\n",
    "        model_ft = models.resnet101(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "\n",
    "    elif model_name == \"MobileNetV2\":\n",
    "        \"\"\" MobileNetV2 (Input 224) \"\"\"\n",
    "        model_ft = models.mobilenet_v2(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[1].in_features\n",
    "        model_ft.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception V3 (Input 299) \"\"\"\n",
    "        # Se requiere aux_logits=True para cargar los pesos preentrenados\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained, aux_logits=True)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Modificar las capas Auxiliar y Principal\n",
    "        model_ft.AuxLogits.fc = nn.Linear(model_ft.AuxLogits.fc.in_features, num_classes)\n",
    "        model_ft.fc = nn.Linear(model_ft.fc.in_features, num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Nombre de modelo inválido.\")\n",
    "        exit()\n",
    "    return model_ft, input_size\n",
    "\n",
    "\n",
    "# La función train_model modificada (indentación corregida)\n",
    "def train_model(model, dataloaders, criterion, optimizer, save_path, num_epochs=30, is_inception=False, model_name=\"\"):\n",
    "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "    since = time.time()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    # Listas para almacenar métricas generales (Epoch-by-Epoch)\n",
    "    general_results_data = []\n",
    "\n",
    "    # Variables para guardar las mejores métricas por especie y la matriz de confusión\n",
    "    best_species_data = []\n",
    "    best_cm_data = None\n",
    "    best_epoch_number = -1\n",
    "\n",
    "    # Nombres de tus 12 especies (clases)\n",
    "    class_names = dataloaders['val'].dataset.classes\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            # INICIO DE LA MEDICIÓN DEL TIEMPO POR FASE\n",
    "            start_phase_time = time.time()\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            all_labels = []\n",
    "            all_preds = []\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4 * loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Acumulación de datos para métricas globales y por clase\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            # FIN DE LA MEDICIÓN DEL TIEMPO POR FASE\n",
    "            time_elapsed_phase = time.time() - start_phase_time\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "\n",
    "            # Métrica General\n",
    "            epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "            epoch_recall_micro = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "            epoch_precision_micro = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "            epoch_f1_score_micro = f1_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f} F1 (micro): {:.4f}'.format(\n",
    "                  phase, epoch_loss, epoch_acc, epoch_f1_score_micro))\n",
    "\n",
    "            # Almacenamiento de Métricas Generales (Para el primer Excel)\n",
    "            general_results_data.append({\n",
    "                'Model': model_name,\n",
    "                'Epoch': epoch,\n",
    "                'Phase': phase,\n",
    "                'Loss': epoch_loss,\n",
    "                'Acc': epoch_acc,\n",
    "                'F1_micro': epoch_f1_score_micro,\n",
    "                'Time_sec': time_elapsed_phase # GUARDANDO EL TIEMPO POR FASE\n",
    "            })\n",
    "\n",
    "            # Actualización del mejor modelo\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_epoch_number = epoch\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                # Guardar pesos del mejor modelo\n",
    "                torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'loss':loss},\n",
    "                           f'{save_path}/best.pt')\n",
    "\n",
    "                # CÁLCULO DE MÉTRICAS DETALLADAS POR ESPECIE (Para el segundo Excel)\n",
    "                report = classification_report(all_labels, all_preds, target_names=class_names, output_dict=True, zero_division=0)\n",
    "\n",
    "                best_species_data = []\n",
    "                for class_name, metrics in report.items():\n",
    "                    if class_name in class_names:\n",
    "                        best_species_data.append({\n",
    "                            'Model': model_name,\n",
    "                            'Best_Acc': best_acc,\n",
    "                            'Species': class_name,\n",
    "                            'Precision': metrics['precision'],\n",
    "                            'Recall': metrics['recall'],\n",
    "                            'F1_Score': metrics['f1-score'],\n",
    "                            'Support': metrics['support']\n",
    "                        })\n",
    "\n",
    "                best_cm_data = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        print('Tiempo total de Época: {:.0f}m {:.0f}s'.format(time_elapsed_phase // 60, time_elapsed_phase % 60)) # Muestra el tiempo de la última fase (val)\n",
    "\n",
    "\n",
    "    time_elapsed_total = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed_total // 60, time_elapsed_total % 60))\n",
    "    print('Best val acc: {:.4f} en época {}'.format(best_acc, best_epoch_number))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Convertir resultados de especies y matriz a DataFrames\n",
    "    best_species_df = pd.DataFrame(best_species_data)\n",
    "\n",
    "    # Matriz de Confusión a DataFrame (Filas: Real, Columnas: Predicción)\n",
    "    if best_cm_data is not None:\n",
    "        best_cm_df = pd.DataFrame(best_cm_data, index=class_names, columns=class_names)\n",
    "        best_cm_df.insert(0, 'Model', model_name)\n",
    "    else:\n",
    "        best_cm_df = pd.DataFrame()\n",
    "\n",
    "    return pd.DataFrame(general_results_data), best_species_df, best_cm_df\n",
    "\n",
    "\n",
    "# --- 1. Bloque de Entrenamiento Comparativo ---\n",
    "\n",
    "\n",
    "# Modelos a entrenar y acumulación de resultados\n",
    "MODELS_TO_RUN = [\n",
    "    {\"name\": \"VGG11-BN\", \"optimizer\": \"SGD\"},\n",
    "    {\"name\": \"DenseNet-201\", \"optimizer\": \"SGD\"},\n",
    "    {\"name\": \"ResNet-101\", \"optimizer\": \"SGD\"},\n",
    "    {\"name\": \"MobileNetV2\", \"optimizer\": \"SGD\"},\n",
    "    {\"name\": \"inception\", \"optimizer\": \"SGD\"},\n",
    "]\n",
    "\n",
    "\n",
    "ALL_GENERAL_RESULTS = []\n",
    "ALL_SPECIES_RESULTS = []\n",
    "ALL_CMS = []\n",
    "\n",
    "\n",
    "num_classes = 12\n",
    "batch_size = 8\n",
    "num_epochs = 30\n",
    "feature_extract = False\n",
    "data_dir = './plantas_dataset'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "for config in MODELS_TO_RUN:\n",
    "    model_name = config['name']\n",
    "\n",
    "\n",
    "    print(f\"\\n{'='*30}\\n INICIANDO ENTRENAMIENTO: {model_name}\\n{'='*30}\")\n",
    "\n",
    "\n",
    "    # Inicialización del modelo para obtener el input_size correcto\n",
    "    model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "\n",
    "    # 2. Transformaciones de Datos (Con las aumentaciones solicitadas)\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.255])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((input_size, input_size)),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.255])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "\n",
    "    # 3. DataLoaders\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "    dataloader_dict = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "    # 4. Optimizador (Usando SGD 0.001 como base de tu estudio)\n",
    "    params_to_update = model_ft.parameters()\n",
    "    optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "    # 5. Entrenamiento\n",
    "    save_path = model_name.replace('-', '_').upper()\n",
    "    is_inception = (model_name == \"inception\")\n",
    "\n",
    "\n",
    "    general_df, species_df, cm_df = train_model(\n",
    "        model_ft, dataloader_dict, criterion, optimizer_ft,\n",
    "        save_path=save_path, num_epochs=num_epochs,\n",
    "        is_inception=is_inception, model_name=model_name\n",
    "    )\n",
    "\n",
    "\n",
    "    # 6. Acumular resultados\n",
    "    ALL_GENERAL_RESULTS.append(general_df)\n",
    "    ALL_SPECIES_RESULTS.append(species_df)\n",
    "    ALL_CMS.append(cm_df)\n",
    "\n",
    "\n",
    "# --- 7. Exportar Resultados a CSV (o Excel) ---\n",
    "\n",
    "\n",
    "# Resultados generales (Epoch-by-Epoch)\n",
    "final_general_df = pd.concat(ALL_GENERAL_RESULTS, ignore_index=True)\n",
    "final_general_df.to_csv('General_Results_History.csv', index=False)\n",
    "\n",
    "\n",
    "# Resultados detallados por especie (Mejor época)\n",
    "final_species_df = pd.concat(ALL_SPECIES_RESULTS, ignore_index=True)\n",
    "final_species_df.to_csv('Species_Metrics_Best_Epoch.csv', index=False)\n",
    "\n",
    "\n",
    "# Matriz de Confusión (Mejor época de cada modelo)\n",
    "# Se guardará en un archivo con una estructura para fácil lectura\n",
    "with pd.ExcelWriter('Confusion_Matrices.xlsx', engine='xlsxwriter') as writer:\n",
    "    for cm_df in ALL_CMS:\n",
    "        if not cm_df.empty:\n",
    "            model_name_safe = cm_df['Model'].iloc[0].replace('-', '_')\n",
    "            cm_df_to_save = cm_df.drop(columns=['Model'])\n",
    "            # Escribir cada matriz en una hoja separada\n",
    "            cm_df_to_save.to_excel(writer, sheet_name=model_name_safe)\n",
    "\n",
    "\n",
    "print(\"\\nExportación finalizada:\\n\")\n",
    "print(\"- 'General_Results_History.csv' (Resultados generales, epoch-by-epoch)\")\n",
    "print(\"- 'Species_Metrics_Best_Epoch.csv' (Resultados por especie de la mejor época)\")\n",
    "print(\"- 'Confusion_Matrices.xlsx' (Matrices de confusión, una hoja por modelo)\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
