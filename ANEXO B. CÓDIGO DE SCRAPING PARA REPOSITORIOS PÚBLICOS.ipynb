{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRIMERO MONTAMOS NUESTRO DRIVE PARA PODER ALMACENAR LAS IMAGENES SCRAPEADAS\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import hashlib\n",
    "import csv\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from math import floor\n",
    "\n",
    "\n",
    "# ===================== CONFIG (ajusta según prefieras) =====================\n",
    "BASE_DIR = \"/content/drive/MyDrive/PLANTS_MEDICAL\"   # carpeta base en Drive\n",
    "# Nombre de la carpeta base del dataset\n",
    "DATASET_NAME = \"dataset_split_anidado\"\n",
    "OUTPUT_ROOT_DIR = os.path.join(BASE_DIR, DATASET_NAME)\n",
    "\n",
    "\n",
    "# Fuentes a usar: 'inaturalist', 'wikimedia'\n",
    "SOURCES = [\"inaturalist\", \"wikimedia\"]\n",
    "\n",
    "\n",
    "MAX_PER_SPECIES = 1200  # máximo imágenes por especie (ajusta)\n",
    "INAT_PER_PAGE = 50\n",
    "PAUSE_BETWEEN_REQS = 0.5\n",
    "TIMEOUT = 15\n",
    "\n",
    "\n",
    "# Porcentajes de división (deben sumar 1.0 o 100)\n",
    "TRAIN_PERCENT = 0.70 # 70%\n",
    "VAL_PERCENT = 0.20   # 20%\n",
    "TEST_PERCENT = 0.10  # 10%\n",
    "\n",
    "\n",
    "if not (TRAIN_PERCENT + VAL_PERCENT + TEST_PERCENT) == 1.0:\n",
    "    print(\"¡ERROR DE CONFIGURACIÓN! Los porcentajes de división no suman 1.0 (100%).\")\n",
    "\n",
    "\n",
    "# Especies (common_name: scientific_name)\n",
    "SPECIES = {\n",
    "    \"Eucalipto\": \"Eucalyptus globulus\",\n",
    "    \"Manzanilla\": \"Matricaria chamomilla\",\n",
    "    \"Puna Salvia\": \"Lepechinia meyenii\",\n",
    "    \"Romero\": \"Rosmarinus officinalis\",\n",
    "    \"Ruda\": \"Ruta graveolens\",\n",
    "    \"Muña\": \"Minthostachys mollis\",\n",
    "    \"Berro\": \"Nasturtium officinale\",\n",
    "    \"Ortiga\": \"Urtica urens\",\n",
    "    \"Llantén\": \"Plantago lanceolata\",\n",
    "    \"Wira Wira\": \"Gnaphalium glandulosum\",\n",
    "    \"Mullaka\": \"Muehlenbeckia volcanica\",\n",
    "    \"Ajenjo\": \"Artemisia absinthium\"\n",
    "}\n",
    "\n",
    "\n",
    "# ===================== UTILIDADES ADAPTADAS =====================\n",
    "def md5_of_bytes(b):\n",
    "    m = hashlib.md5()\n",
    "    m.update(b)\n",
    "    return m.hexdigest()\n",
    "\n",
    "\n",
    "def safe_filename(name):\n",
    "    return \"\".join(c if c.isalnum() or c in \"._-\" else \"_\" for c in name).strip()\n",
    "\n",
    "\n",
    "def ensure_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_split_dir(common_name):\n",
    "    \"\"\"\n",
    "    Asigna la ruta de guardado a la carpeta train, val o test y crea la subcarpeta de especie anidada.\n",
    "    Estructura: OUTPUT_ROOT_DIR / split / common_name_split /\n",
    "    \"\"\"\n",
    "    r = random.random() # Número aleatorio entre 0.0 y 1.0\n",
    "\n",
    "\n",
    "    # 1. Determinar el split (train/val/test)\n",
    "    if r < TRAIN_PERCENT:\n",
    "        split = \"train\"\n",
    "    elif r < (TRAIN_PERCENT + VAL_PERCENT):\n",
    "        split = \"val\"\n",
    "    else:\n",
    "        split = \"test\"\n",
    "\n",
    "\n",
    "    # 2. Crear el nombre de la subcarpeta de la especie anidada (ej: Eucalipto_train)\n",
    "    nested_species_dir_name = safe_filename(f\"{common_name}_{split}\")\n",
    "\n",
    "\n",
    "    # 3. Construir la ruta final: OUTPUT_ROOT_DIR / split / nested_species_dir_name\n",
    "    final_dir = os.path.join(OUTPUT_ROOT_DIR, split, nested_species_dir_name)\n",
    "\n",
    "\n",
    "    ensure_dir(final_dir) # Aseguramos que exista la carpeta\n",
    "    return final_dir, split\n",
    "\n",
    "\n",
    "# ===================== PREPARAR CSV DE METADATOS =====================\n",
    "ensure_dir(OUTPUT_ROOT_DIR)\n",
    "\n",
    "\n",
    "# Creamos las carpetas train, val, test vacías si no existen, dentro de OUTPUT_ROOT_DIR\n",
    "ensure_dir(os.path.join(OUTPUT_ROOT_DIR, \"train\"))\n",
    "ensure_dir(os.path.join(OUTPUT_ROOT_DIR, \"val\"))\n",
    "ensure_dir(os.path.join(OUTPUT_ROOT_DIR, \"test\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "csv_path = os.path.join(OUTPUT_ROOT_DIR, \"descargas_metadata_split_anidado.csv\")\n",
    "# Si el archivo existe, lo SOBREESCRIBIMOS\n",
    "csv_file = open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "csv_writer = csv.writer(csv_file)\n",
    "# Añadimos la columna 'split'\n",
    "csv_writer.writerow([\"common_name\",\"scientific_name\",\"source\",\"source_id\",\"image_url\",\"saved_path\",\"md5\",\"split\"])\n",
    "\n",
    "\n",
    "# ===================== DESCARGAR DESDE iNATURALIST =====================\n",
    "# La lógica interna de descarga es la misma, solo cambia cómo se llama a get_split_dir\n",
    "def download_from_inaturalist(common_name, scientific_name, max_images):\n",
    "    \"\"\"Descarga observaciones de iNaturalist con división train/val/test.\"\"\"\n",
    "    count = 0\n",
    "    page = 1\n",
    "    base_api = \"https://api.inaturalist.org/v1/observations\"\n",
    "    params = {\n",
    "        \"taxon_name\": scientific_name,\n",
    "        \"quality_grade\": \"research\",\n",
    "        \"photos\": True,\n",
    "        \"per_page\": INAT_PER_PAGE,\n",
    "        \"page\": page\n",
    "    }\n",
    "\n",
    "\n",
    "    while count < max_images:\n",
    "        params[\"page\"] = page\n",
    "        try:\n",
    "            r = requests.get(base_api, params=params, timeout=TIMEOUT)\n",
    "            data = r.json()\n",
    "        except Exception as e:\n",
    "            print(\"Error iNaturalist API:\", e)\n",
    "            break\n",
    "\n",
    "\n",
    "        results = data.get(\"results\", [])\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "\n",
    "        for obs in results:\n",
    "            photos = obs.get(\"photos\", [])\n",
    "            for ph in photos:\n",
    "                if count >= max_images:\n",
    "                    break\n",
    "\n",
    "\n",
    "                url = ph.get(\"url\")\n",
    "                if not url:\n",
    "                    continue\n",
    "                url = url.replace(\"square\", \"large\")\n",
    "                try:\n",
    "                    resp = requests.get(url, timeout=TIMEOUT)\n",
    "                    if resp.status_code != 200:\n",
    "                        continue\n",
    "                    b = resp.content\n",
    "                    h = md5_of_bytes(b)\n",
    "\n",
    "\n",
    "                    # OBTENER CARPETA DE DIVISIÓN (estructura anidada)\n",
    "                    target_dir, split_name = get_split_dir(common_name)\n",
    "\n",
    "\n",
    "                    # Guardar\n",
    "                    fname = safe_filename(f\"{scientific_name.replace(' ','_')}_inat_{count}.jpg\")\n",
    "                    outp = os.path.join(target_dir, fname)\n",
    "                    with open(outp, \"wb\") as fh:\n",
    "                        fh.write(b)\n",
    "                    # Escribir metadatos\n",
    "                    csv_writer.writerow([common_name, scientific_name, \"iNaturalist\", obs.get(\"id\"), url, outp, h, split_name])\n",
    "                    count += 1\n",
    "\n",
    "\n",
    "                except Exception:\n",
    "                    continue\n",
    "                time.sleep(PAUSE_BETWEEN_REQS)\n",
    "            if count >= max_images:\n",
    "                break\n",
    "\n",
    "\n",
    "        # paginación\n",
    "        total_results = data.get(\"total_results\", 0)\n",
    "        if (page * INAT_PER_PAGE) >= total_results:\n",
    "            break\n",
    "        page += 1\n",
    "        time.sleep(PAUSE_BETWEEN_REQS)\n",
    "    return count\n",
    "\n",
    "\n",
    "# ===================== DESCARGAR DESDE WIKIMEDIA COMMONS =====================\n",
    "# La lógica interna de descarga es la misma, solo cambia cómo se llama a get_split_dir\n",
    "def download_from_wikimedia(common_name, scientific_name, max_images, start_count):\n",
    "    \"\"\"Busca imágenes en Wikimedia Commons con división train/val/test.\"\"\"\n",
    "    count = start_count\n",
    "    session = requests.Session()\n",
    "    search_api = \"https://commons.wikimedia.org/w/api.php\"\n",
    "    query = scientific_name\n",
    "    params_search = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"generator\": \"search\",\n",
    "        \"gsrsearch\": query,\n",
    "        \"gsrlimit\": 50,\n",
    "        \"prop\": \"imageinfo\",\n",
    "        \"iiprop\": \"url\",\n",
    "        \"iiurlwidth\": 800\n",
    "    }\n",
    "    try:\n",
    "        r = session.get(search_api, params=params_search, timeout=TIMEOUT)\n",
    "        data = r.json()\n",
    "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "    except Exception as e:\n",
    "        print(\"Wikimedia search error:\", e)\n",
    "        return 0\n",
    "\n",
    "\n",
    "    for pid, page in pages.items():\n",
    "        if count >= max_images:\n",
    "            break\n",
    "\n",
    "\n",
    "        imageinfo = page.get(\"imageinfo\")\n",
    "        if not imageinfo:\n",
    "            continue\n",
    "        for info in imageinfo:\n",
    "            if count >= max_images:\n",
    "                break\n",
    "\n",
    "\n",
    "            url = info.get(\"thumburl\") or info.get(\"url\")\n",
    "            if not url:\n",
    "                continue\n",
    "            try:\n",
    "                resp = requests.get(url, timeout=TIMEOUT)\n",
    "                if resp.status_code != 200:\n",
    "                    continue\n",
    "                b = resp.content\n",
    "                h = md5_of_bytes(b)\n",
    "\n",
    "\n",
    "                # OBTENER CARPETA DE DIVISIÓN (estructura anidada)\n",
    "                target_dir, split_name = get_split_dir(common_name)\n",
    "\n",
    "\n",
    "                # Guardar\n",
    "                fname = safe_filename(f\"{scientific_name.replace(' ','_')}_wm_{count}.jpg\")\n",
    "                outp = os.path.join(target_dir, fname)\n",
    "                with open(outp, \"wb\") as fh:\n",
    "                    fh.write(b)\n",
    "                # Escribir metadatos\n",
    "                csv_writer.writerow([common_name, scientific_name, \"Wikimedia\", pid, url, outp, h, split_name])\n",
    "                count += 1\n",
    "\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "            time.sleep(PAUSE_BETWEEN_REQS)\n",
    "\n",
    "\n",
    "    return count - start_count\n",
    "\n",
    "\n",
    "# ===================== MAIN: ejecutar descargas por especie =====================\n",
    "summary = []\n",
    "\n",
    "\n",
    "# Configuramos la semilla para que la división sea reproducible\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "for common, sci in SPECIES.items():\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Procesando: {common}  ({sci})\")\n",
    "    print(\"=============================\")\n",
    "\n",
    "\n",
    "    got = 0\n",
    "    # priorizar iNaturalist\n",
    "    if \"inaturalist\" in SOURCES:\n",
    "        need = MAX_PER_SPECIES - got\n",
    "        if need > 0:\n",
    "            n = download_from_inaturalist(common, sci, need)\n",
    "            got += n\n",
    "            print(f\"  iNaturalist: +{n}\")\n",
    "\n",
    "\n",
    "    # fallback wikimedia si faltan\n",
    "    if got < MAX_PER_SPECIES and \"wikimedia\" in SOURCES:\n",
    "        need = MAX_PER_SPECIES - got\n",
    "        n = download_from_wikimedia(common, sci, MAX_PER_SPECIES, got)\n",
    "        got += n\n",
    "        print(f\"  Wikimedia: +{n}\")\n",
    "\n",
    "\n",
    "    summary.append((common, sci, got))\n",
    "    print(f\"Total obtenidas para {common}: {got}/{MAX_PER_SPECIES}\")\n",
    "\n",
    "\n",
    "# Cerrar CSV\n",
    "csv_file.close()\n",
    "\n",
    "\n",
    "print(\"\\n====== RESUMEN FINAL ======\")\n",
    "for s in summary:\n",
    "    print(f\"- {s[0]} ({s[1]}): {s[2]} imágenes\")\n",
    "\n",
    "\n",
    "print(\"\\nMetadatos guardados en:\", csv_path)\n",
    "print(\"Carpeta destino:\", OUTPUT_ROOT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
